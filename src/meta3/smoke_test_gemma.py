import pandas as pd
import torch
from transformers import pipeline, AutoTokenizer, BitsAndBytesConfig
from sklearn.metrics import classification_report, accuracy_score
import time
import platform

# CONFIGURAÃ‡Ã•ES
# Link direto do Hugging Face (Requer login via terminal)
DATASET_URL = "hf://datasets/franciellevargas/HateBR/HateBR.csv" 

# ConfiguraÃ§Ãµes do teste
AMOSTRA_TAMANHO = 20  # None = corre o dataset todo (demorado!)
MODEL_ID = "google/gemma-2-2b-it" 
# MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.3"
# MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"

print(f"--> A iniciar pipeline com o modelo: {MODEL_ID}")
print(f"--> Sistema detetado: {platform.system()} | Processador: {platform.processor()}")

# DETEÃ‡ÃƒO DE HARDWARE (Universal Mac/Windows)
model_kwargs = {}

if torch.cuda.is_available():
    # MODO WINDOWS (Colega com NVIDIA)
    print("âœ… Hardware: NVIDIA GPU detetada (Modo CUDA).")
    print("--> Ativando compressÃ£o 4-bit para mÃ¡xima eficiÃªncia.")
    nf4_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16
    )
    model_kwargs = {"quantization_config": nf4_config, "low_cpu_mem_usage": True}

elif torch.backends.mps.is_available():
    # MODO MAC
    print("ðŸŽ Hardware: Apple Silicon detetado (Modo MPS/Metal).")
    print("--> Ativando modo nativo float16.")
    model_kwargs = {"torch_dtype": torch.float16, "low_cpu_mem_usage": True}

else:
    # MODO CPU (EmergÃªncia)
    print("âš ï¸ Hardware: Apenas CPU detetado. Vai ser lento!")
    model_kwargs = {"low_cpu_mem_usage": True}

# --- 2. CARREGAR DADOS (Direto do Hugging Face) ---
print(f"--> A descarregar dataset de: {DATASET_URL}...")
try:
    # O pandas lÃª direto do URL usando a autenticaÃ§Ã£o do teu 'huggingface-cli login'
    df = pd.read_csv(DATASET_URL)
    
    # SeleÃ§Ã£o das colunas
    df = df[['comentario', 'label_final']].rename(
        columns={'comentario': 'texto', 'label_final': 'label_real'}
    )
    
    # Mapeamento para texto (facilita a comparaÃ§Ã£o com o LLM)
    df['label_texto'] = df['label_real'].map({0: 'NÃ£o-ofensivo', 1: 'Ofensivo'})
    
    if AMOSTRA_TAMANHO:
        df = df.sample(n=AMOSTRA_TAMANHO, random_state=42)
        
        # Mostra quantas de cada tipo apanhÃ¡mos para garantir que estÃ¡ equilibrado
        print(f"--> DistribuiÃ§Ã£o da amostra:\n{df['label_texto'].value_counts()}")
    print(f"--> Dados carregados: {len(df)} linhas.")
    
except Exception as e:
    print(f"âŒ Erro ao ler dataset: {e}")
    print("Dica: Verifica se fizeste 'huggingface-cli login' no terminal.")
    exit()

#3. CARREGAR MODELO
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    pipe = pipeline(
        "text-generation",
        model=MODEL_ID,
        model_kwargs=model_kwargs,
        device_map="auto", 
    )
    print("--> Modelo carregado com sucesso!")
except Exception as e:
    print(f"âŒ Erro crÃ­tico ao carregar modelo: {e}")
    exit()

#4. ENGENHARIA DE PROMPT
def gerar_prompt(comentario):
    # Prompt Few-Shot (Com exemplos)
    messages = [
        {"role": "user", "content": f"""
        Analisa o comentÃ¡rio abaixo e classifica como 'Ofensivo' ou 'NÃ£o-ofensivo'.
        
        Exemplos:
        ComentÃ¡rio: "Adorei a foto, estÃ¡s linda!"
        ClassificaÃ§Ã£o: NÃ£o-ofensivo

        ComentÃ¡rio: "Que burra, nem devias estar aqui."
        ClassificaÃ§Ã£o: Ofensivo
        
        ComentÃ¡rio: "{comentario}"
        ClassificaÃ§Ã£o:"""}
    ]
    return messages

def limpar_resposta(texto_gerado):
    # FunÃ§Ã£o auxiliar para limpar a resposta do modelo
    texto = texto_gerado.lower()
    if "nÃ£o-ofensivo" in texto or "nÃ£o ofensivo" in texto:
        return "NÃ£o-ofensivo"
    elif "ofensivo" in texto:
        return "Ofensivo"
    else:
        return "Erro"

# 5. EXECUÃ‡ÃƒO DO TESTE
print("\n--> A classificar comentÃ¡rios (isto pode demorar um pouco)...")
start_time = time.time()

predicoes = []

for i, texto in enumerate(df['texto']):
    # Mostra progresso a cada 5 linhas
    if i % 5 == 0: print(f"Processando linha {i}/{len(df)}...")
    
    # 1. Gerar prompt
    prompt = gerar_prompt(texto)
    
    # 2. Chamar modelo
    saida = pipe(prompt, max_new_tokens=10, do_sample=False, pad_token_id=tokenizer.eos_token_id)
    
    # 3. Processar resposta
    resposta_crua = saida[0]["generated_text"][-1]["content"]
    resposta_limpa = limpar_resposta(resposta_crua)
    
    predicoes.append(resposta_limpa)

df['predicao'] = predicoes
tempo_total = time.time() - start_time

# 6. RESULTADOS
print("\n" + "="*30)
print(f"MODELO: {MODEL_ID}")
print(f"TEMPO: {tempo_total:.2f} segundos")
print("="*30)

# Filtrar erros para cÃ¡lculo de mÃ©tricas
df_validos = df[df['predicao'] != "Erro"]
erros_count = len(df) - len(df_validos)

if len(df_validos) > 0:
    print(f"AcurÃ¡cia: {accuracy_score(df_validos['label_texto'], df_validos['predicao']):.2f}")
    print(f"Erros de formataÃ§Ã£o (respostas invÃ¡lidas): {erros_count}")
    print("\nRelatÃ³rio Detalhado:")
    print(classification_report(df_validos['label_texto'], df_validos['predicao']))
else:
    print("âš ï¸ O modelo nÃ£o gerou nenhuma resposta vÃ¡lida (verificar Prompt).")

# Guardar resultados
nome_csv = f"resultados_{MODEL_ID.split('/')[-1]}.csv"
df.to_csv(nome_csv, index=False)
print(f"--> Resultados guardados em: {nome_csv}")