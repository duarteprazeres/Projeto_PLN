import pandas as pd
import torch
from transformers import pipeline, AutoTokenizer, BitsAndBytesConfig
from sklearn.metrics import classification_report, accuracy_score
import time
import platform

# --- CONFIGURA√á√ïES GERAIS ---
DATASET_URL = "hf://datasets/franciellevargas/HateBR/HateBR.csv" 
AMOSTRA_TAMANHO = 20  # Mant√©m 20 para testes r√°pidos. P√µe 50 ou 100 para o relat√≥rio final.
MODEL_ID = "google/gemma-2-2b-it" 

print(f"--> A iniciar pipeline com o modelo: {MODEL_ID}")
print(f"--> Sistema: {platform.system()} | Processador: {platform.processor()}")

# --- 1. CONFIGURA√á√ÉO DE HARDWARE (Universal) ---
model_kwargs = {}
if torch.cuda.is_available():
    print("‚úÖ Hardware: NVIDIA GPU (CUDA). Modo 4-bit ativado.")
    nf4_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16
    )
    model_kwargs = {"quantization_config": nf4_config, "low_cpu_mem_usage": True}
elif torch.backends.mps.is_available():
    print("üçé Hardware: Apple Silicon (MPS). Modo float16 ativado.")
    model_kwargs = {"torch_dtype": torch.float16, "low_cpu_mem_usage": True}
else:
    print("‚ö†Ô∏è Hardware: CPU. Vai ser lento.")
    model_kwargs = {"low_cpu_mem_usage": True}

# --- 2. CARREGAR DADOS ---
print(f"--> A carregar dados...")
try:
    df_original = pd.read_csv(DATASET_URL)
    # Ajuste das colunas conforme verific√°mos antes
    df_original = df_original[['comentario', 'label_final']].rename(
        columns={'comentario': 'texto', 'label_final': 'label_real'}
    )
    df_original['label_texto'] = df_original['label_real'].map({0: 'N√£o-ofensivo', 1: 'Ofensivo'})
    
    if AMOSTRA_TAMANHO:
        # Importante: Amostragem aleat√≥ria para evitar vi√©s
        df = df_original.sample(n=AMOSTRA_TAMANHO, random_state=42).copy()
        print(f"--> Amostra aleat√≥ria carregada: {len(df)} linhas.")
        print(f"--> Distribui√ß√£o: {df['label_texto'].value_counts().to_dict()}")
    else:
        df = df_original.copy()
        
except Exception as e:
    print(f"‚ùå Erro ao ler dataset: {e}")
    exit()

# --- 3. CARREGAR MODELO ---
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    pipe = pipeline("text-generation", model=MODEL_ID, model_kwargs=model_kwargs, device_map="auto")
    print("--> Modelo pronto!")
except Exception as e:
    print(f"‚ùå Erro no modelo: {e}")
    exit()

# --- 4. DEFINI√á√ÉO DAS 4 PROMPTS (A Evolu√ß√£o) ---

def get_prompt_1_zeroshot(texto):
    # Prompt 1: B√°sica e direta (Baseline)
    return [{"role": "user", "content": f"""
    Classifica o coment√°rio abaixo como 'Ofensivo' ou 'N√£o-ofensivo'.
    Responde apenas com a etiqueta.
    
    Coment√°rio: "{texto}"
    Classifica√ß√£o:"""}]

def get_prompt_2_fewshot_basic(texto):
    # Prompt 2: Few-Shot (Ensina o formato)
    return [{"role": "user", "content": f"""
    Classifica o coment√°rio como 'Ofensivo' ou 'N√£o-ofensivo'.
    
    Exemplos:
    Coment√°rio: "Adoro este lugar!" -> Classifica√ß√£o: N√£o-ofensivo
    Coment√°rio: "Seu idiota in√∫til." -> Classifica√ß√£o: Ofensivo
    
    Coment√°rio: "{texto}"
    Classifica√ß√£o:"""}]

def get_prompt_3_fewshot_advanced(texto):
    # Prompt 3: Few-Shot Focado em Casos Dif√≠ceis (Ironia/Cr√≠tica vs √ìdio)
    return [{"role": "user", "content": f"""
    Analisa o coment√°rio para detetar discurso de √≥dio.
    Distingue entre cr√≠tica v√°lida (N√£o-ofensivo) e insultos (Ofensivo).
    
    Exemplos:
    "N√£o concordo nada contigo." -> N√£o-ofensivo
    "√âs uma vergonha para a humanidade." -> Ofensivo
    "O servi√ßo foi p√©ssimo." -> N√£o-ofensivo
    
    Coment√°rio: "{texto}"
    Classifica√ß√£o:"""}]

def get_prompt_4_persona(texto):
    # Prompt 4: Persona (Content Moderator) + Regras Estritas
    return [{"role": "user", "content": f"""
    Tu √©s um Moderador de Conte√∫do de uma rede social.
    A tua tarefa √© identificar toxicidade.
    - Se for apenas opini√£o negativa, marca 'N√£o-ofensivo'.
    - Se tiver insultos ou ataques pessoais, marca 'Ofensivo'.
    
    Coment√°rio: "{texto}"
    Classifica√ß√£o (Ofensivo/N√£o-ofensivo):"""}]

# Lista de prompts para o loop
prompts_list = [
    (1, get_prompt_1_zeroshot),
    (2, get_prompt_2_fewshot_basic),
    (3, get_prompt_3_fewshot_advanced),
    (4, get_prompt_4_persona)
]

def limpar_resposta(texto_gerado):
    t = texto_gerado.lower()
    if "n√£o-ofensivo" in t or "n√£o ofensivo" in t: return "N√£o-ofensivo"
    if "ofensivo" in t: return "Ofensivo"
    return "Erro"

# --- 5. EXECU√á√ÉO EM LOOP ---

for num_prompt, func_prompt in prompts_list:
    print(f"\nüöÄ A executar PROMPT {num_prompt}...")
    
    predicoes = []
    start_time = time.time()
    
    for i, row in df.iterrows():
        # Gerar a prompt espec√≠fica desta rodada
        messages = func_prompt(row['texto'])
        
        # Infer√™ncia
        saida = pipe(messages, max_new_tokens=10, do_sample=False, pad_token_id=tokenizer.eos_token_id)
        res_limpa = limpar_resposta(saida[0]["generated_text"][-1]["content"])
        predicoes.append(res_limpa)
        
        # Feedback visual simples (ponto a cada infer√™ncia)
        print(".", end="", flush=True)
    
    # Guardar resultados desta prompt numa coluna tempor√°ria para c√°lculo
    col_name = f"pred_prompt_{num_prompt}"
    df[col_name] = predicoes
    
    # M√©tricas
    # 1. Calcular acur√°cia REAL (considerando "Erro" como falha)
    # Usamos o df completo, n√£o o filtrado
    acc = accuracy_score(df['label_texto'], df[col_name])
    
    # 2. Contar quantos erros de formata√ß√£o ocorreram
    num_erros_formato = len(df[df[col_name] == "Erro"])
    
    print(f"\n‚úÖ Prompt {num_prompt} terminada!")
    print(f"   Acur√°cia Real: {acc:.2f}")
    print(f"   Respostas Inv√°lidas (Erro): {num_erros_formato}/{len(df)}")
    
    # --- GUARDAR CSV INDIVIDUAL ---
    nome_modelo_limpo = MODEL_ID.split('/')[-1]
    nome_ficheiro = f"resultados_{num_prompt}_{nome_modelo_limpo}.csv"
    
    # Guardamos apenas as colunas relevantes para este ficheiro
    df_final = df[['texto', 'label_real', 'label_texto', col_name]].copy()
    df_final.rename(columns={col_name: 'predicao'}, inplace=True)
    
    path_guardar = f"src/meta3/{nome_ficheiro}"
    df_final.to_csv(path_guardar, index=False)
    print(f"üíæ Ficheiro guardado: {path_guardar}")

print("\nüèÅ Processo conclu√≠do para todas as 4 prompts!")